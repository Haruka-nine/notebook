
详细代码解读和讲解网站
https://ailearning.apachecn.org/#/

# k-近邻算法

## 概述

k 近邻算法假设给定一个训练数据集，其中的实例类别已定。
分类时，对新的实例，根据其 k 个最近邻的训练实例的类别，通过多数表决等方式进行预测。
因此，k近邻算法不具有显式的学习过程。

## KNN原理

>工作原理


1.  假设有一个带有标签的样本数据集（训练样本集），其中包含每条数据与所属分类的对应关系。
2.  输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较。
    1.  计算新数据与样本数据集中每条数据的距离。
    2.  对求得的所有距离进行排序（从小到大，越小表示越相似）。
    3.  取前 k （k 一般小于等于 20 ）个样本数据对应的分类标签。
3.  求 k 个数据中出现次数最多的分类标签作为新数据的分类。

>通俗理解

给定一个训练数据集，对新的输入实例，在训练数据集中找到与该实例最邻近的 k 个实例，这 k 个实例的多数属于某个类，就把该输入实例分为这个类。

>开发流程

收集数据: 任何方法
准备数据: 距离计算所需要的数值，最好是结构化的数据格式
分析数据: 任何方法
训练算法: 此步骤不适用于 k-近邻算法
测试算法: 计算错误率
使用算法: 输入样本数据和结构化的输出结果，然后运行 k-近邻算法判断输入数据分类属于哪个分类，最后对计算出的分类执行后续处理

>算法特点

优点: 精度高、对异常值不敏感、无数据输入假定 缺点: 计算复杂度高、空间复杂度高 适用数据范围: 数值型和标称型

## 归一化数据
![[Pasted image 20230314172235.png]]

面对这样的样本数据，如果我们直接进行计算
![[Pasted image 20230314172307.png]]

很显然，飞行里程占的比重过大，但可能这三个属性应该同样重要
我们就需要 对其进行归一化，就是求出每个元素的最大最小值，求出差，然后每个元素除差。

```ad-note
title:总结
k近邻算法是分类数据最简单有效的算法
使用算法是我们必须有接近实际数据的训练样本数据  
k近邻算法必须保存全部数据集，如果训练数据集很大，就必须**使用大量的存储空间**  
此外由于必须对每个数据计算距离值，实际使用可能十分**耗时**
```


# 决策树

## 概述

决策树（Decision Tree）算法是一种基本的分类与回归方法，是最经常使用的数据挖掘算法之一。我们这章节只讨论用于分类的决策树

决策树模型呈树形结构，在分类问题中，表示基于特征对实例进行分类的过程。它可以认为是 if-then 规则的集合，也可以认为是定义在特征空间与类空间上的条件概率分布。

决策树学习通常包括 3 个步骤:***特征选择、决策树的生成和决策树的修剪***

示例：
![[Pasted image 20230315203753.png]]

像这样的流程

```
首先检测发送邮件域名地址。如果地址为 myEmployer.com, 则将其放在分类 "无聊时需要阅读的邮件"中。
如果邮件不是来自这个域名，则检测邮件内容里是否包含单词 "曲棍球" , 如果包含则将邮件归类到 "需要及时处理的朋友邮件", 
如果不包含则将邮件归类到 "无需阅读的垃圾邮件" 。
```

---
## 原理

>须知概念

熵（entropy）: 熵指的是体系的混乱的程度，在不同的学科中也有引申出的更为具体的定义，是各领域十分重要的参量。

信息论（information theory）中的熵（香农熵）: 是一种信息的度量方式，表示信息的混乱程度，也就是说: 信息越有序，信息熵越低。例如: 火柴有序放在火柴盒里，熵值很低，相反，熵值很高。

信息增益（information gain）: 在划分数据集前后信息发生的变化称为信息增益。

>开发流程

收集数据: 可以使用任何方法。 
准备数据: 树构造算法 (这里使用的是ID3算法，只适用于标称型数据，这就是为什么数值型数据必须离散化。 还有其他的树构造算法，比如CART) 
分析数据: 可以使用任何方法，构造树完成之后，我们应该检查图形是否符合预期。 
训练算法: 构造树的数据结构。 
测试算法: 使用训练好的树计算错误率。
使用算法: 此步骤可以适用于任何监督学习任务，而使用决策树可以更好地理解数据的内在含义。

>算法特点

优点: 计算复杂度不高，输出结果易于理解，数据有缺失也能跑，可以处理不相关特征。 
缺点: 容易过拟合。 
适用数据类型: 数值型和标称型。


## 案例分析

>概述

根据以下 2 个特征，将动物分成两类: 鱼类和非鱼类。
特征:
1.  不浮出水面是否可以生存
2.  是否有脚蹼

>收集数据：可以使用任何方法

![[Pasted image 20230315204318.png]]


>准备数据：树构造算法

此处，由于我们输入的数据本身就是离散化数据，所以这一步就省略了。

>分析数据：可以使用任何方法，构造树完成后，我们可以将树画出来

![[Pasted image 20230315204442.png]]


首先，设计函数来计算香农熵  （1）

我们意识到要根据树进行划分，那么先判断哪个特征更好呢？
依据就是香农熵，经过划分后熵越小，混乱度越小，那么这个特征就是适合进行划分的特征

然后我们设计一个函数来计算当前dateSet的最好特征

然后我们开始创建树，去根据最好特征进行划分，然后再递归调用这个函数，直到特征用完，或者在这个时候剩下的类型已经全是一致的

这个时候跑出来的树就是我们的算法，我们只需要保存这个树。


# 朴素贝叶斯

## 概述

贝叶斯分类是一类分类算法的总称，这类算法均以贝叶斯定理为基础，故统称为贝叶斯分类。本章首先介绍贝叶斯分类算法的基础——贝叶斯定理。最后，我们通过实例来讨论贝叶斯分类的中最简单的一种: 朴素贝叶斯分类。

```ad-note
优点: 在数据较少的情况下仍然有效，可以处理多类别问题。 
缺点: 对于输入数据的准备方式较为敏感。 
适用数据类型: 标称型数据。
```

## 贝叶斯理论&条件概率

### 贝叶斯理论
现在有一个数据集，由两类数据构成，数据分布如下

![[Pasted image 20230323162647.png]]

 p1(x,y) 表示数据点 (x,y) 属于类别 1（图中用圆点表示的类别）的概率
 
 p2(x,y) 表示数据点 (x,y) 属于类别 2（图中三角形表示的类别）的概率

如果p1>p2,那么（x,y）就属于类别1，反之则属于类别2

也就是说，我们会选择高概率对应的类别。这就是贝叶斯决策理论的核心思想，**即选择具有最高概率的决策。**
### 条件概率


![[Pasted image 20230323163053.png]]

![[Pasted image 20230323163106.png]]


第二张图就是条件概率

在A桶或者在B桶这个条件的前提下取出，是会改变结果的

我们可以很容易的算出：P(white|bucketA) 值为 2/4 ，P(white|bucketB) 的值为 1/3

条件概率的计算公式如下:

P(white|bucketB) = P(white and bucketB) / P(bucketB)

首先，我们用 B 桶中白色石头的个数除以两个桶中总的石头数，得到 P(white and bucketB) = 1/7 .其次，由于 B 桶中有 3 块石头，而总石头数为 7 ，于是 P(bucketB) 就等于 3/7 。于是又 P(white|bucketB) = P(white and bucketB) / P(bucketB) = (1/7) / (3/7) = 1/3 。

另外一种有效计算条件概率的方法称为贝叶斯准则。贝叶斯准则告诉我们如何交换条件概率中的条件与结果，即如果已知 P(x|c)，要求 P(c|x)，那么可以使用下面的计算方法:

![[Pasted image 20230323163336.png]]

### 使用条件概率来分类

上面我们提到贝叶斯决策理论要求计算两个概率 p1(x, y) 和 p2(x, y): * 如果 p1(x, y) > p2(x, y), 那么属于类别 1; * 如果 p2(x, y) > p1(X, y), 那么属于类别 2.

这并不是贝叶斯决策理论的所有内容。使用 p1() 和 p2() 只是为了尽可能简化描述，而真正需要计算和比较的是 p(c1|x, y) 和 p(c2|x, y) .这些符号所代表的具体意义是: 给定某个由 x、y 表示的数据点，那么该数据点来自类别 c1 的概率是多少？数据点来自类别 c2 的概率又是多少？注意这些概率与概率 p(x, y|c1) 并不一样，不过可以使用贝叶斯准则来交换概率中条件与结果。具体地，应用贝叶斯准则得到:

![[Pasted image 20230323163651.png]]

使用上面这些定义，可以定义贝叶斯分类准则为: **如果 P(c1|x, y) > P(c2|x, y), 那么属于类别 c1;如果 P(c2|x, y) > P(c1|x, y), 那么属于类别 c2.**

在文档分类中，整个文档（如一封电子邮件）是实例，而电子邮件中的某些元素则构成特征。我们可以观察文档中出现的词，并把每个词作为一个特征，而每个词的出现或者不出现作为该特征的值，这样得到的特征数目就会跟词汇表中的词的数目一样多。

我们假设特征之间 **相互独立** 。所谓 **独立(independence)** 指的是统计意义上的独立，即一个特征或者单词出现的可能性与它和其他单词相邻没有关系，比如说，“我们”中的“我”和“们”出现的概率与这两个字相邻没有任何关系。这个假设正是朴素贝叶斯分类器中 朴素(naive) 一词的含义。朴素贝叶斯分类器中的另一个假设是，**每个特征同等重要**。

```ad-note
 朴素贝叶斯分类器通常有两种实现方式: 一种基于伯努利模型实现，一种基于多项式模型实现。这里采用前一种实现方式。该实现方式中并不考虑词在文档中出现的次数，只考虑出不出现，因此在这个意义上相当于假设词是等权重的。
```


## 案例分析

### 1.屏蔽侮辱言论

>将文字转换为向量

```python
def loadDataSet():
    """
    创建数据集
    :return: 单词列表postingList, 所属类别classVec
    """
    postingList = [['my', 'dog', 'has', 'flea', 'problems', 'help', 'please'], 
                   ['maybe', 'not', 'take', 'him', 'to', 'dog', 'park', 'stupid'],
                   ['my', 'dalmation', 'is', 'so', 'cute', 'I', 'love', 'him'],
                   ['stop', 'posting', 'stupid', 'worthless', 'garbage'],
                   ['mr', 'licks', 'ate', 'my', 'steak', 'how', 'to', 'stop', 'him'],
                   ['quit', 'buying', 'worthless', 'dog', 'food', 'stupid']]
    classVec = [0, 1, 0, 1, 0, 1]  # 1 is abusive, 0 not
    return postingList, classVec

```

单词列表postingList，其中每个元素都是一个文档

我们将postingList中所有词组成一个不重复的set，然后将每个文档转化为一个set长度的list，文档中存在这个词，就在对应的这个词的位置设置为1.

```ad-note
这里也可以看出来朴素贝叶斯，不考虑单词的出现次数，只考虑是否出现，认为其相互独立
```

单词set为如下
```
>>> myVocabList ['cute', 'love', 'help', 'garbage', 'quit', 'I', 'problems', 'is', 'park', 'stop', 'flea', 'dalmation', 'licks', 'food', 'not', 'him', 'buying', 'posting', 'has', 'worthless', 'ate', 'to', 'maybe', 'please', 'dog', 'how', 'stupid', 'so', 'take', 'mr', 'steak', 'my']
```

将文档转化为向量
```
>>> bayes.setOfWords2Vec(myVocabList, listOPosts[0])
[0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1]

>>> bayes.setOfWords2Vec(myVocabList, listOPosts[3])
[0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0]

```


>训练算法:从词向量计算概率

现在已经知道了一个词是否出现在一篇文档中，也知道该文档所属的类别。接下来我们重写贝叶斯准则，将之前的 x, y 替换为 **w**. 粗体的 **w** 表示这是一个向量，即它由多个值组成。在这个例子中，数值个数与词汇表中的词个数相同。

![[Pasted image 20230323171044.png]]

我们使用上述公式，对每个类计算该值，然后比较这两个概率值的大小。

```ad-note
问: 上述代码实现中，为什么没有计算P(w)？

答: 根据上述公式可知，我们右边的式子等同于左边的式子，由于对于每个ci，P(w)是固定的。并且我们只需要比较左边式子值的大小来决策分类，那么我们就可以简化为通过比较右边分子值得大小来做决策分类。
```

首先可以通过类别 i (侮辱性留言或者非侮辱性留言)中的文档数除以总的文档数来计算概率 p(ci) 。
接下来计算 p(**w** | ci) ，这里就要用到朴素贝叶斯假设。如果将 w 展开为一个个独立特征，那么就可以将上述概率写作 p(w0, w1, w2...wn | ci) 。这里假设所有词都互相独立，该假设也称作条件独立性假设（例如 A 和 B 两个人抛骰子，概率是互不影响的，也就是相互独立的，A 抛 2点的同时 B 抛 3 点的概率就是 1/6 * 1/6），它意味着可以使用 p(w0 | ci)p(w1 | ci)p(w2 | ci)...p(wn | ci) 来计算上述概率，这样就极大地简化了计算的过程。

这样我们分别求出了侮辱性文档和非侮辱性的 p(c1)和 p(w0 | ci)p(w1 | ci)p(w2 | ci)...p(wn | ci)

然后相乘求出概率比较就可以了

但这里有两个问题
```ad-note
title:问题1
有的词的概率为0所以，P(w0|c1)就是0，但因为是相乘，导致整个概率全是0
那么我们就对其设置一个初始都有一个1（具体看代码改）
```

```ad-note
title:问题2
因为有的概率很小，导致一直乘就会导致无限接近于0
![[Pasted image 20230323213529.png]]
这时我们可以对其取ln，因为一起增大，所以并不影响最终结果并且使结果更好
```


